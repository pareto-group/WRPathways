{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from itertools import product\n",
    "import hexMinisom\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import colormaps\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.feature as cf\n",
    "import pickle\n",
    "import itertools\n",
    "import colorsys\n",
    "import random\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_som(som, fileName):\n",
    "    with open(fileName, 'wb') as outfile:\n",
    "        pickle.dump(som, outfile)\n",
    "\n",
    "def load_som(fileName):\n",
    "    with open(fileName, 'rb') as infile:\n",
    "        som = pickle.load(infile)\n",
    "    return som\n",
    "\n",
    "def generate_distinct_colors(n):\n",
    "    colors = []\n",
    "    \n",
    "    # Generate `n` distinct colors in the HSV color space\n",
    "    for i in range(n):\n",
    "        # Generate a unique hue for each color, evenly spaced between 0 and 1\n",
    "        hue = i / n\n",
    "        # Use full saturation and value to get vibrant colors\n",
    "        saturation = 1.0\n",
    "        value = 1.0\n",
    "        \n",
    "        # Convert HSV to RGB (the result is a tuple of RGB values in [0, 1])\n",
    "        rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "        \n",
    "        # Convert RGB to a format that ranges from 0 to 255 and create a hex string\n",
    "        rgb = [int(x * 255) for x in rgb]\n",
    "        hex_color = f\"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}\"\n",
    "        colors.append(hex_color)\n",
    "\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "# Get the counts of each WR_label for each node in SOM\n",
    "def get_WR_counts(l, return_percents=False, indices=None):\n",
    "\n",
    "    # Take only the data from days with the given indices in indices\n",
    "    if indices is None:\n",
    "        labels, counts = np.unique([WR_labels[i] for i in l], return_counts=True)\n",
    "    else:\n",
    "        labels, counts = np.unique([WR_labels[i] for i in l if i in indices], return_counts=True)\n",
    "\n",
    "    # If the node has no days of a given regime add the regime to the labels and 0 as its count\n",
    "    if len(labels) != len(WR_labels_dict.keys()):\n",
    "        missing_values = np.setdiff1d(list(WR_labels_dict.keys()), labels)\n",
    "\n",
    "        labels = WR_labels_dict.keys()\n",
    "\n",
    "        for v in missing_values:\n",
    "           counts = np.insert(counts, v, 0)\n",
    "\n",
    "    # Calculate the percents\n",
    "    if return_percents:\n",
    "        counts = 100 * counts / sum(counts)\n",
    "\n",
    "    regime_counts = dict(zip(labels, counts))\n",
    "    return regime_counts\n",
    "\n",
    "# Plotting functions\n",
    "def hex_heatmap(som, data, cmap='Blues', title='', cbLabel=''):\n",
    "    # set up the figure\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect('equal')\n",
    "    cmap = mpl.colormaps[cmap]\n",
    "    \n",
    "    # get data from the som\n",
    "    weights = som.get_weights()\n",
    "    xx, yy = som.get_euclidean_coordinates()\n",
    "    \n",
    "    maxCount = max(v for v in data.values())\n",
    "    #maxCount = 553  #frequency\n",
    "    #maxCount = 4.39   #persistence\n",
    "    minCount = min(v for v in data.values())\n",
    "    #minCount = 65    #frequency\n",
    "    #minCount = 1.23   #persistence\n",
    "    \n",
    "    # loops through the neurons\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            # Only use non-masked nodes\n",
    "            if som._mask[i, j] == 0:\n",
    "                # If theres no data still plot the hexagon\n",
    "                if (i, j) not in data:\n",
    "                    data[(i,j)] = 0\n",
    "                    \n",
    "                # determine the hexagon position and color\n",
    "                wy = yy[(j, i)] * np.sqrt(3) / 2\n",
    "                colorWeight = data[(i, j)]/maxCount\n",
    "                \n",
    "                # Create hexagon and add it to axis\n",
    "                hex = patches.RegularPolygon((xx[(j, i)], wy), numVertices=6, radius=.85 / np.sqrt(3), \n",
    "                                        facecolor=cmap(colorWeight), edgecolor='grey')\n",
    "                ax.add_patch(hex)\n",
    "                \n",
    "                # determine the color the text should be based on color of node\n",
    "                if colorWeight >= .75:\n",
    "                    textColor = 'white'\n",
    "                else:\n",
    "                    textColor = 'black'\n",
    "                \n",
    "                # add text to hexagon for its frequency\n",
    "                plt.text(xx[(j, i)], wy - .07, f'{data[(i, j)]}', {'horizontalalignment': 'center', 'color': textColor, 'fontsize': 17})\n",
    "            \n",
    "    # align figure to show all hexagons\n",
    "    plt.xlim(-1, weights.shape[0] - .5)\n",
    "    plt.ylim(-1, (weights.shape[1] - .5) * np.sqrt(3) / 2)\n",
    "    \n",
    "    # remove the axis labels and lines\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create the color bar\n",
    "    norm = mpl.colors.Normalize(vmin=minCount, vmax=maxCount)\n",
    "    cb = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, location='bottom', anchor=(.56, 2.3), shrink=.65, extend='both')\n",
    "    cb.set_label(cbLabel, fontsize=17)\n",
    "    cb.ax.tick_params(labelsize=17)\n",
    "    \n",
    "    # Title the plot\n",
    "    plt.title(title, fontsize=17, y=.95, x=.515)\n",
    "    return fig\n",
    "\n",
    "def hex_frequency_plot(som, winmap=None):\n",
    "\n",
    "    if winmap == None:\n",
    "        winmap = som.win_map(dataarray)\n",
    "    \n",
    "    data = {k: len(v) for k, v in winmap.items()}\n",
    "    \n",
    "    fig = hex_heatmap(som, data, 'Blues', '(a) SOM Node Frequencies', 'Count')\n",
    "    return fig\n",
    "\n",
    "def hex_plot(som, projection=None):\n",
    "    \"\"\"Create a matplot lib figure with an axis for each neuron already positioned into the hexagonal shape\"\"\"\n",
    "    \n",
    "    # Extract the needed data from the som\n",
    "    n = som._num\n",
    "    xy = (2 * n) - 1\n",
    "    mask = som._mask\n",
    "    node_indices_xy = np.ma.where(mask == False)\n",
    "    node_indices = list(zip(node_indices_xy[0], node_indices_xy[1]))\n",
    "\n",
    "    # create figure\n",
    "    totRows = xy * 3\n",
    "    totCols = xy * 2\n",
    "    fig = plt.figure(figsize=[totRows, totCols])\n",
    "    axs = {}\n",
    "    \n",
    "    for x, y in list(product(range(xy), range(xy))):\n",
    "\n",
    "        # make a subplot for the nodes not masked\n",
    "        if (y, x) in node_indices: # for showing in the proper orientation x and y must be switched\n",
    "            \n",
    "            # odd rows will be offset to keep the hexagonal shape\n",
    "            if y % 2 == 0:\n",
    "                curRow = x * 3\n",
    "            else:\n",
    "                curRow = (x * 3) + 1\n",
    "                \n",
    "            curCol = (totCols - 2) - (y * 2)\n",
    "\n",
    "            ax = plt.subplot2grid((totCols, totRows), (curCol, curRow), rowspan=2, colspan=2, projection=projection)\n",
    "            ax.set_title(node_nums[(y, x)])\n",
    "            \n",
    "            if projection is not None:\n",
    "                ax.set_extent([-180, -30, 20, 80], crs=projection)\n",
    "            axs[(y, x)] = ax\n",
    "    return fig, axs\n",
    "\n",
    "def find_longest_consecutive_index(arr):\n",
    "    if not arr.size:\n",
    "        return -1  # Handle empty array\n",
    "\n",
    "    max_len = 0\n",
    "    start_index = -1\n",
    "    current_len = 1\n",
    "    current_start = 0\n",
    "\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] == arr[i - 1]:\n",
    "            current_len += 1\n",
    "        else:\n",
    "            if current_len > max_len:\n",
    "                max_len = current_len\n",
    "                start_index = current_start\n",
    "            current_len = 1\n",
    "            current_start = i\n",
    "\n",
    "    # Check if the last sequence is the longest\n",
    "    if current_len > max_len:\n",
    "        start_index = current_start\n",
    "    return start_index\n",
    "\n",
    "def trend_significance(group, alpha=.05):\n",
    "    g = list(group)\n",
    "    slopes = []\n",
    "    iters = 10000\n",
    "    \n",
    "    for i in range(iters):\n",
    "        samples = []\n",
    "        for j in range(len(group)):\n",
    "            samples.extend(random.sample(g, 1))\n",
    "            \n",
    "        m = np.polyfit(group.index, samples, 1)[0]\n",
    "        slopes.append(m)\n",
    "        \n",
    "    lower = np.percentile(slopes, (alpha/2) * 100)\n",
    "    upper = np.percentile(slopes, (1 - alpha/2) * 100)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_spread(winmap, y, n_classes):\n",
    "    \"\"\"\n",
    "    For each class:\n",
    "    - Get BMU coordinates of all samples\n",
    "    - Compute the centroid\n",
    "    - Compute average Euclidean distance to centroid\n",
    "    \"\"\"\n",
    "    class_spread = {}\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        bmu_coords = []\n",
    "\n",
    "        # Get all BMUs for class cls\n",
    "        for bmu, indices in winmap.items():\n",
    "            for i in indices:\n",
    "                if y[i] == cls:\n",
    "                    bmu_coords.append(np.array(bmu))\n",
    "\n",
    "        if not bmu_coords:\n",
    "            class_spread[cls] = 0.0\n",
    "            continue\n",
    "\n",
    "        bmu_coords = np.array(bmu_coords)\n",
    "        centroid = bmu_coords.mean(axis=0)\n",
    "        distances = np.linalg.norm(bmu_coords - centroid, axis=1)\n",
    "        spread = np.mean(distances)\n",
    "        class_spread[cls] = spread\n",
    "    return class_spread\n",
    "\n",
    "def compute_class_coverage(winmap, y, n_classes):\n",
    "    \"\"\"\n",
    "    For each class, count how many unique SOM nodes are activated.\n",
    "    \"\"\"\n",
    "    class_nodes = defaultdict(set)\n",
    "\n",
    "    for bmu, indices in winmap.items():\n",
    "        for i in indices:\n",
    "            class_nodes[y[i]].add(bmu)\n",
    "\n",
    "    class_coverage = {cls: len(class_nodes[cls]) for cls in range(n_classes)}\n",
    "    return class_coverage\n",
    "    \n",
    "def compute_class_entropy(winmap, y, n_classes):\n",
    "    \"\"\"\n",
    "    Computes entropy of SOM node distribution for each class.\n",
    "    Entropy reflects how spread out each class is across the SOM.\n",
    "    \"\"\"\n",
    "    class_entropy = {}\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        node_counts = []\n",
    "        total = 0\n",
    "\n",
    "        for bmu, indices in winmap.items():\n",
    "            count = sum(1 for i in indices if y[i] == cls)\n",
    "            if count > 0:\n",
    "                node_counts.append(count)\n",
    "                total += count\n",
    "\n",
    "        if total == 0:\n",
    "            class_entropy[cls] = 0.0\n",
    "            continue\n",
    "\n",
    "        probs = [count / total for count in node_counts]\n",
    "        entropy = -sum(p * log(p) for p in probs if p > 0)\n",
    "        class_entropy[cls] = entropy\n",
    "\n",
    "    return class_entropy\n",
    "    \n",
    "def compute_topographic_class_purity(winmap, y, n_classes):\n",
    "    \"\"\"\n",
    "    Computes the average node purity for each class.\n",
    "    Purity = proportion of the most frequent class in each node.\n",
    "    \"\"\"\n",
    "    class_purity = {}\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        purities = []\n",
    "\n",
    "        for bmu, indices in winmap.items():\n",
    "            labels = [y[i] for i in indices]\n",
    "            if cls in labels:\n",
    "                counts = Counter(labels)\n",
    "                node_purity = counts[cls] / len(labels)\n",
    "                purities.append(node_purity)\n",
    "\n",
    "        if purities:\n",
    "            class_purity[cls] = sum(purities) / len(purities)\n",
    "        else:\n",
    "            class_purity[cls] = 0.0\n",
    "    return class_purity\n",
    "\n",
    "def check_node_activation_by_class(winmap, y):\n",
    "    \n",
    "    class_node_count = defaultdict(set)\n",
    "\n",
    "    for bmu, indices in winmap.items():\n",
    "        labels_in_node = set(y[i] for i in indices)\n",
    "        for label in labels_in_node:\n",
    "            class_node_count[label].add(bmu)\n",
    "\n",
    "    for cls in sorted(class_node_count.keys()):\n",
    "        print(f\"Class {cls} appears in {len(class_node_count[cls])} SOM nodes\")\n",
    "\n",
    "def compute_weighted_coverage(winmap, y, n_classes, method='effective'):\n",
    "    \"\"\"\n",
    "    Compute weighted coverage of classes over SOM nodes.\n",
    "    \n",
    "    method:\n",
    "      - 'effective': exponential of entropy of class distribution over nodes (effective number of nodes)\n",
    "      - 'simpson': inverse of sum of squared proportions (Simpson index)\n",
    "    \n",
    "    Returns:\n",
    "      dict[class] -> weighted coverage score (higher = more spread out)\n",
    "    \"\"\"\n",
    "    class_node_counts = {cls: defaultdict(int) for cls in range(n_classes)}\n",
    "    class_totals = {cls: 0 for cls in range(n_classes)}\n",
    "    \n",
    "    # Count samples of each class in each node\n",
    "    for bmu, indices in winmap.items():\n",
    "        for i in indices:\n",
    "            cls = y[i]\n",
    "            class_node_counts[cls][bmu] += 1\n",
    "            class_totals[cls] += 1\n",
    "    \n",
    "    coverage = {}\n",
    "    for cls in range(n_classes):\n",
    "        counts = np.array(list(class_node_counts[cls].values()))\n",
    "        if counts.size == 0:\n",
    "            coverage[cls] = 0.0\n",
    "            continue\n",
    "        p = counts / counts.sum()  # proportion per node\n",
    "        \n",
    "        if method == 'effective':\n",
    "            entropy = -np.sum(p * np.log(p + 1e-12))  # add small value to avoid log(0)\n",
    "            coverage[cls] = np.exp(entropy)\n",
    "        elif method == 'simpson':\n",
    "            coverage[cls] = 1 / np.sum(p ** 2)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'effective' or 'simpson'\")\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30660, 9211)\n"
     ]
    }
   ],
   "source": [
    "dataset = xr.open_dataarray('/glade/work/molina/DATA/Z500Anoms_ERA5.nc')\n",
    "\n",
    "latSlice = slice(20, 80) #20N, 80N\n",
    "lonSlice = slice(180, 330) #180W, 30W\n",
    "dataarray = dataset.sel(lat=latSlice, lon=lonSlice)\n",
    "dataarray = dataarray.stack(latlon=['lat', 'lon']).values\n",
    "\n",
    "# Seasonal breakdown of the data\n",
    "DJF = dataset.time.dt.month.isin([12, 1, 2])\n",
    "DJF_idxs = np.array(DJF).nonzero()[0]\n",
    "MAM = dataset.time.dt.month.isin([3, 4, 5])\n",
    "MAM_idxs = np.array(MAM).nonzero()[0]\n",
    "JJA = dataset.time.dt.month.isin([6, 7, 8])\n",
    "JJA_idxs = np.array(JJA).nonzero()[0]\n",
    "SON = dataset.time.dt.month.isin([9, 10, 11])\n",
    "SON_idxs = np.array(SON).nonzero()[0]\n",
    "\n",
    "print(dataarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = load_som('SOM40.p')\n",
    "\n",
    "n = som._num\n",
    "xy = hexMinisom.xy_using_n(n)\n",
    "\n",
    "mask = som._mask\n",
    "node_indices_xy = np.ma.where(mask == False)\n",
    "node_indices = list(zip(node_indices_xy[0], node_indices_xy[1]))\n",
    "all_nodes = product(range(xy), range(xy))\n",
    "\n",
    "inputLength = dataarray.shape[1]\n",
    "\n",
    "winmap = som.win_map(dataarray, return_indices=True)\n",
    "# Seasonal breakdown for winmap\n",
    "DJF_winmap = {}\n",
    "MAM_winmap = {}\n",
    "JJA_winmap = {}\n",
    "SON_winmap = {}\n",
    "\n",
    "# Loop through each node\n",
    "for k, v in winmap.items():\n",
    "    \n",
    "    # Keep only the days that are in the given season\n",
    "    DJF_winmap[k] = [i for i in v if i in DJF_idxs]\n",
    "    MAM_winmap[k] = [i for i in v if i in MAM_idxs]\n",
    "    JJA_winmap[k] = [i for i in v if i in JJA_idxs]\n",
    "    SON_winmap[k] = [i for i in v if i in SON_idxs]\n",
    "\n",
    "w = som._weights\n",
    "minimum_weight = -np.max(np.abs(w))\n",
    "maximum_weight = np.max(np.abs(w))\n",
    "\n",
    "# Calculate the node number for each coordinate\n",
    "node_nums = {}\n",
    "n = 1\n",
    "for i in range(mask.shape[0])[::-1]:\n",
    "    for j in range(mask.shape[1]):\n",
    "        # only use non masked nodes\n",
    "        if som._mask[i, j] == 0:\n",
    "            node_nums[(i, j)] = n\n",
    "            n += 1\n",
    "\n",
    "color_list = generate_distinct_colors(len(node_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date  WR  distances      corr\n",
      "0     1940-01-01   0   2.463938  0.518457\n",
      "1     1940-01-02   0   2.662645  0.565398\n",
      "2     1940-01-03   0   2.916932  0.552532\n",
      "3     1940-01-04   0   3.122750  0.495652\n",
      "4     1940-01-05   0   3.302769  0.394692\n",
      "...          ...  ..        ...       ...\n",
      "30655 2023-12-27   1   2.823370  0.857459\n",
      "30656 2023-12-28   1   2.687266  0.837698\n",
      "30657 2023-12-29   1   2.394127  0.781903\n",
      "30658 2023-12-30   1   2.253624  0.655454\n",
      "30659 2023-12-31   0   2.468174  0.359541\n",
      "\n",
      "[30660 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import the regime labels\n",
    "WR_labels_df = pd.read_csv('df_labels_nocorrfilt_ERA5.csv')\n",
    "WR_labels_df.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "WR_labels_df['date'] = pd.to_datetime(WR_labels_df['date'], format='%Y-%m-%d')\n",
    "WR_labels_dict = {\n",
    "    0: 'Polar High', 1: 'Pacific Trough', 2: 'Pacific Ridge', \n",
    "    3: 'Alaskan Ridge', 4: 'Atlantic Ridge', 5: 'No WR'\n",
    "}\n",
    "WR_labels = np.array(WR_labels_df['WR'])\n",
    "\n",
    "WRs_by_node = {k: get_WR_counts(v) for k, v in winmap.items()}\n",
    "WRs_percents = {k: get_WR_counts(v, return_percents=True) for k, v in winmap.items()}\n",
    "\n",
    "print(WR_labels_df)\n",
    "\n",
    "# Calculate the 90th percentile of the distances and only keep data less than that\n",
    "percentile90 = np.percentile(WR_labels_df['distances'], 90)\n",
    "lt90 = (np.array(WR_labels_df['distances']) < percentile90).nonzero()[0]\n",
    "WRs_lt90 = {k: get_WR_counts(v, True, lt90) for k, v in winmap.items()}\n",
    "\n",
    "# Calculate the variances of the distances for each WR\n",
    "WR_indices = {i: (WR_labels == i).nonzero()[0] for i in np.unique(WR_labels)}\n",
    "for WR, idxs in WR_indices.items():\n",
    "    variance = np.var(WR_labels_df['distances'].iloc[idxs])\n",
    "    \n",
    "# Get the WR counts for each specific season\n",
    "WRs_DJF = {k: get_WR_counts(v, True, DJF_idxs) for k, v in winmap.items()}\n",
    "WRs_MAM = {k: get_WR_counts(v, True, MAM_idxs) for k, v in winmap.items()}\n",
    "WRs_JJA = {k: get_WR_counts(v, True, JJA_idxs) for k, v in winmap.items()}\n",
    "WRs_SON = {k: get_WR_counts(v, True, SON_idxs) for k, v in winmap.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_fill = []\n",
    "\n",
    "for i in WR_labels_df['WR'].values:\n",
    "    \n",
    "    names_to_fill.append(WR_labels_dict[i])\n",
    "\n",
    "array_to_fill = np.ones(len(WR_labels_df), dtype=int) * -9999\n",
    "\n",
    "for i in range(0, len(node_indices)):\n",
    "    \n",
    "    for j in winmap[node_indices[i]]:\n",
    "        \n",
    "        array_to_fill[j] = node_nums[node_indices[i]]\n",
    "\n",
    "WR_labels_df['node'] = array_to_fill\n",
    "WR_labels_df['WR_name'] = names_to_fill\n",
    "\n",
    "diversity_df = pd.concat([WR_labels_df, pd.DataFrame(dataarray)], axis=1)\n",
    "\n",
    "# Extract feature columns — these are the SOM inputs\n",
    "X = diversity_df.iloc[:, 6:]\n",
    "\n",
    "# Extract class labels (e.g. WR categories)\n",
    "y = diversity_df['WR'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5]), array([5217, 6164, 5943, 4279, 5821, 3236]))\n",
      "30660\n",
      "Coverage = 36, Simpson = 10.57: Polar High\n",
      "Coverage = 37, Simpson = 21.48: Pacific Trough\n",
      "Coverage = 37, Simpson = 13.22: Pacific Ridge\n",
      "Coverage = 37, Simpson = 12.38: Alaskan Ridge\n",
      "Coverage = 37, Simpson = 30.28: Atlantic Ridge\n",
      "Coverage = 37, Simpson = 30.41: No WR\n"
     ]
    }
   ],
   "source": [
    "# total samples per regime\n",
    "print(np.unique(y, return_counts=True))\n",
    "print(len(y))\n",
    "\n",
    "tmp_map = winmap\n",
    "\n",
    "spread_scores = compute_class_spread(tmp_map, y, n_classes=6)\n",
    "coverage_scores = compute_class_coverage(tmp_map, y, n_classes=6)\n",
    "entropy_scores = compute_class_entropy(tmp_map, y, n_classes=6)\n",
    "purity_scores = compute_topographic_class_purity(tmp_map, y, n_classes=6)\n",
    "wcoverage_effective = compute_weighted_coverage(tmp_map, y, n_classes=6, method='effective')\n",
    "wcoverage_simpson = compute_weighted_coverage(tmp_map, y, n_classes=6, method='simpson')\n",
    "\n",
    "#for cls in range(6):\n",
    "#    print(\n",
    "#        f\"Spread = {\n",
    "#        spread_scores[cls]:.3f}, Coverage = {\n",
    "#        coverage_scores[cls]}, Entropy = {\n",
    "#        entropy_scores[cls]:.3f}, Purity = {\n",
    "#        purity_scores[cls]:.3f}, Eff. Coverage = {\n",
    "#        wcoverage_effective[cls]:.3f}, Simpson = {\n",
    "#        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "#    )\n",
    "\n",
    "for cls in range(6):\n",
    "    print(\n",
    "        f\"Coverage = {\n",
    "        coverage_scores[cls]}, Simpson = {\n",
    "        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5]), array([1186, 1636, 1748,  889, 1352,  749]))\n",
      "7560\n",
      "Coverage = 31, Simpson = 8.60: Polar High\n",
      "Coverage = 37, Simpson = 18.06: Pacific Trough\n",
      "Coverage = 32, Simpson = 11.53: Pacific Ridge\n",
      "Coverage = 34, Simpson = 8.69: Alaskan Ridge\n",
      "Coverage = 37, Simpson = 27.84: Atlantic Ridge\n",
      "Coverage = 37, Simpson = 26.17: No WR\n"
     ]
    }
   ],
   "source": [
    "# total samples per regime\n",
    "print(np.unique(y[DJF_idxs], return_counts=True))\n",
    "print(len(y[DJF_idxs]))\n",
    "\n",
    "tmp_map = DJF_winmap\n",
    "\n",
    "spread_scores = compute_class_spread(tmp_map, y, n_classes=6)\n",
    "coverage_scores = compute_class_coverage(tmp_map, y, n_classes=6)\n",
    "entropy_scores = compute_class_entropy(tmp_map, y, n_classes=6)\n",
    "purity_scores = compute_topographic_class_purity(tmp_map, y, n_classes=6)\n",
    "wcoverage_effective = compute_weighted_coverage(tmp_map, y, n_classes=6, method='effective')\n",
    "wcoverage_simpson = compute_weighted_coverage(tmp_map, y, n_classes=6, method='simpson')\n",
    "\n",
    "#for cls in range(6):\n",
    "#    print(\n",
    "#        f\"Spread = {\n",
    "#        spread_scores[cls]:.3f}, Coverage = {\n",
    "#        coverage_scores[cls]}, Entropy = {\n",
    "#        entropy_scores[cls]:.3f}, Purity = {\n",
    "#        purity_scores[cls]:.3f}, Eff. Coverage = {\n",
    "#        wcoverage_effective[cls]:.3f}, Simpson = {\n",
    "#        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "#    )\n",
    "\n",
    "for cls in range(6):\n",
    "    print(\n",
    "        f\"Coverage = {\n",
    "        coverage_scores[cls]}, Simpson = {\n",
    "        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5]), array([1386, 1553, 1538,  999, 1485,  767]))\n",
      "7728\n",
      "Coverage = 32, Simpson = 9.11: Polar High\n",
      "Coverage = 37, Simpson = 23.05: Pacific Trough\n",
      "Coverage = 36, Simpson = 11.98: Pacific Ridge\n",
      "Coverage = 37, Simpson = 11.01: Alaskan Ridge\n",
      "Coverage = 37, Simpson = 26.55: Atlantic Ridge\n",
      "Coverage = 36, Simpson = 29.22: No WR\n"
     ]
    }
   ],
   "source": [
    "# total samples per regime\n",
    "print(np.unique(y[MAM_idxs], return_counts=True))\n",
    "print(len(y[MAM_idxs]))\n",
    "\n",
    "tmp_map = MAM_winmap\n",
    "\n",
    "spread_scores = compute_class_spread(tmp_map, y, n_classes=6)\n",
    "coverage_scores = compute_class_coverage(tmp_map, y, n_classes=6)\n",
    "entropy_scores = compute_class_entropy(tmp_map, y, n_classes=6)\n",
    "purity_scores = compute_topographic_class_purity(tmp_map, y, n_classes=6)\n",
    "wcoverage_effective = compute_weighted_coverage(tmp_map, y, n_classes=6, method='effective')\n",
    "wcoverage_simpson = compute_weighted_coverage(tmp_map, y, n_classes=6, method='simpson')\n",
    "\n",
    "#for cls in range(6):\n",
    "#    print(\n",
    "#        f\"Spread = {\n",
    "#        spread_scores[cls]:.3f}, Coverage = {\n",
    "#        coverage_scores[cls]}, Entropy = {\n",
    "#        entropy_scores[cls]:.3f}, Purity = {\n",
    "#        purity_scores[cls]:.3f}, Eff. Coverage = {\n",
    "#        wcoverage_effective[cls]:.3f}, Simpson = {\n",
    "#        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "#    )\n",
    "\n",
    "for cls in range(6):\n",
    "    print(\n",
    "        f\"Coverage = {\n",
    "        coverage_scores[cls]}, Simpson = {\n",
    "        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5]), array([1647, 1335, 1060, 1410, 1406,  870]))\n",
      "7728\n",
      "Coverage = 36, Simpson = 11.24: Polar High\n",
      "Coverage = 37, Simpson = 23.61: Pacific Trough\n",
      "Coverage = 37, Simpson = 16.04: Pacific Ridge\n",
      "Coverage = 37, Simpson = 16.08: Alaskan Ridge\n",
      "Coverage = 37, Simpson = 28.24: Atlantic Ridge\n",
      "Coverage = 37, Simpson = 30.51: No WR\n"
     ]
    }
   ],
   "source": [
    "# total samples per regime\n",
    "print(np.unique(y[JJA_idxs], return_counts=True))\n",
    "print(len(y[JJA_idxs]))\n",
    "\n",
    "tmp_map = JJA_winmap\n",
    "\n",
    "spread_scores = compute_class_spread(tmp_map, y, n_classes=6)\n",
    "coverage_scores = compute_class_coverage(tmp_map, y, n_classes=6)\n",
    "entropy_scores = compute_class_entropy(tmp_map, y, n_classes=6)\n",
    "purity_scores = compute_topographic_class_purity(tmp_map, y, n_classes=6)\n",
    "wcoverage_effective = compute_weighted_coverage(tmp_map, y, n_classes=6, method='effective')\n",
    "wcoverage_simpson = compute_weighted_coverage(tmp_map, y, n_classes=6, method='simpson')\n",
    "\n",
    "#for cls in range(6):\n",
    "#    print(\n",
    "#        f\"Spread = {\n",
    "#        spread_scores[cls]:.3f}, Coverage = {\n",
    "#        coverage_scores[cls]}, Entropy = {\n",
    "#        entropy_scores[cls]:.3f}, Purity = {\n",
    "#        purity_scores[cls]:.3f}, Eff. Coverage = {\n",
    "#        wcoverage_effective[cls]:.3f}, Simpson = {\n",
    "#        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "#    )\n",
    "\n",
    "for cls in range(6):\n",
    "    print(\n",
    "        f\"Coverage = {\n",
    "        coverage_scores[cls]}, Simpson = {\n",
    "        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5]), array([ 998, 1640, 1597,  981, 1578,  850]))\n",
      "7644\n",
      "Coverage = 29, Simpson = 12.40: Polar High\n",
      "Coverage = 37, Simpson = 19.93: Pacific Trough\n",
      "Coverage = 35, Simpson = 13.01: Pacific Ridge\n",
      "Coverage = 37, Simpson = 10.30: Alaskan Ridge\n",
      "Coverage = 37, Simpson = 30.00: Atlantic Ridge\n",
      "Coverage = 37, Simpson = 29.01: No WR\n"
     ]
    }
   ],
   "source": [
    "# total samples per regime\n",
    "print(np.unique(y[SON_idxs], return_counts=True))\n",
    "print(len(y[SON_idxs]))\n",
    "\n",
    "tmp_map = SON_winmap\n",
    "\n",
    "spread_scores = compute_class_spread(tmp_map, y, n_classes=6)\n",
    "coverage_scores = compute_class_coverage(tmp_map, y, n_classes=6)\n",
    "entropy_scores = compute_class_entropy(tmp_map, y, n_classes=6)\n",
    "purity_scores = compute_topographic_class_purity(tmp_map, y, n_classes=6)\n",
    "wcoverage_effective = compute_weighted_coverage(tmp_map, y, n_classes=6, method='effective')\n",
    "wcoverage_simpson = compute_weighted_coverage(tmp_map, y, n_classes=6, method='simpson')\n",
    "\n",
    "#for cls in range(6):\n",
    "#    print(\n",
    "#        f\"Spread = {\n",
    "#        spread_scores[cls]:.3f}, Coverage = {\n",
    "#        coverage_scores[cls]}, Entropy = {\n",
    "#        entropy_scores[cls]:.3f}, Purity = {\n",
    "#        purity_scores[cls]:.3f}, Eff. Coverage = {\n",
    "#        wcoverage_effective[cls]:.3f}, Simpson = {\n",
    "#        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "#    )\n",
    "\n",
    "for cls in range(6):\n",
    "    print(\n",
    "        f\"Coverage = {\n",
    "        coverage_scores[cls]}, Simpson = {\n",
    "        wcoverage_simpson[cls]:.2f}: {WR_labels_dict[cls]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wrpathsenv]",
   "language": "python",
   "name": "conda-env-wrpathsenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
